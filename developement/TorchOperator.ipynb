{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pylops - torch operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: M.Ravasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will show how to use the `TorchOperator` to mix and match pylops and pytorch operators into an AD-friendy chain of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "from pylops.torchoperator import TorchOperator\n",
    "from pylops.basicoperators import *\n",
    "from pylops.signalprocessing import Convolve2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 10])\n",
      "Input:  tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "AD gradient:  tensor([-3.5634,  1.5422, -1.1205,  3.1014, -0.7585, -0.0989,  2.0274,  1.0850,\n",
      "         0.2566, -1.8949], dtype=torch.float64)\n",
      "Analytical gradient:  tensor([-3.5634,  1.5422, -1.1205,  3.1014, -0.7585, -0.0989,  2.0274,  1.0850,\n",
      "         0.2566, -1.8949], dtype=torch.float64, grad_fn=<MvBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "nx, ny = 10, 6\n",
    "x0 = torch.arange(nx, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx))\n",
    "Aop = TorchOperator(MatrixMult(A))\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "\n",
    "# AD\n",
    "v = torch.ones(ny, dtype=torch.double)\n",
    "y.backward(v, retain_graph=True)\n",
    "adgrad = x0.grad\n",
    "\n",
    "# Analytical\n",
    "At = torch.from_numpy(A)\n",
    "#J = (At * torch.cos(x0))\n",
    "J = (At * torch.cos(x0))\n",
    "print(J.shape)\n",
    "anagrad = torch.matmul(J.T, v)\n",
    "\n",
    "print('Input: ', x0)\n",
    "print('AD gradient: ', adgrad)\n",
    "print('Analytical gradient: ', anagrad)\n",
    "\n",
    "# Grad check\n",
    "input = (torch.arange(nx, dtype=torch.double, requires_grad=True),\n",
    "         Aop.matvec, Aop.rmatvec, Aop.device, 'cpu')\n",
    "test = gradcheck(Aop.Top, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi batch, we should get here to sum of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD gradient:  tensor([[-2.1960, -0.3267, -1.5877],\n",
      "        [ 2.1741,  0.3953,  1.0822],\n",
      "        [-2.1086, -0.4559, -0.5551],\n",
      "        [ 2.0009,  0.5074,  0.0169],\n",
      "        [-1.8531, -0.5487,  0.5217]])\n",
      "Analytical gradient:  tensor([[-2.1960, -0.3267, -1.5877],\n",
      "        [ 2.1741,  0.3953,  1.0822],\n",
      "        [-2.1086, -0.4559, -0.5551],\n",
      "        [ 2.0009,  0.5074,  0.0169],\n",
      "        [-1.8531, -0.5487,  0.5217]])\n"
     ]
    }
   ],
   "source": [
    "nbatch, nx, ny = 5, 3, 6\n",
    "x0 = torch.arange(nbatch * nx, dtype=torch.float).reshape(nbatch, nx)\n",
    "x0.requires_grad=True\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx)).astype(np.float32)\n",
    "Aop = TorchOperator(MatrixMult(A), batch=True)\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "\n",
    "# AD\n",
    "v = torch.ones((nbatch, ny), dtype=torch.float32)\n",
    "y.backward(v, retain_graph=True)\n",
    "adgrad = x0.grad\n",
    "print('AD gradient: ', adgrad)\n",
    "\n",
    "# Analytical\n",
    "x0.grad.data.zero_()\n",
    "At = torch.from_numpy(A)\n",
    "Lin = nn.Linear(nx, ny, bias=False)\n",
    "Lin.weight.data[:] = At.float()\n",
    "y1 = Lin(torch.sin(x0))\n",
    "y1.backward(v, retain_graph=True)\n",
    "anagrad = x0.grad\n",
    "\n",
    "print('Analytical gradient: ', anagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD gradient:  tensor([[ 0.2593,  0.4270, -0.0529],\n",
      "        [ 0.1449,  0.4336, -0.0390],\n",
      "        [ 0.0292,  0.3944, -0.0212],\n",
      "        [-0.0784,  0.3124, -0.0007],\n",
      "        [-0.1695,  0.1943,  0.0208]])\n",
      "Analytical gradient:  tensor([[ 0.2593,  0.4270, -0.0529],\n",
      "        [ 0.1449,  0.4336, -0.0390],\n",
      "        [ 0.0292,  0.3944, -0.0212],\n",
      "        [-0.0784,  0.3124, -0.0007],\n",
      "        [-0.1695,  0.1943,  0.0208]])\n"
     ]
    }
   ],
   "source": [
    "nbatch, nx, ny = 5, 3, 6\n",
    "x0 = torch.arange(nbatch*nx, dtype=torch.float).reshape(nbatch, nx).requires_grad_()\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx)).astype(np.float32)\n",
    "Aop = TorchOperator(MatrixMult(A), batch=True)\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "l = torch.mean(y**2)\n",
    "l.backward()\n",
    "adgrad = x0.grad\n",
    "print('AD gradient: ', adgrad)\n",
    "\n",
    "# Analytical\n",
    "x1 = torch.arange(nbatch*nx, dtype=torch.float).reshape(nbatch, nx).requires_grad_()\n",
    "At = torch.from_numpy(A)\n",
    "Lin = nn.Linear(nx, ny, bias=False)\n",
    "Lin.weight.data[:] = At.float()\n",
    "y1 = Lin(torch.sin(x1))\n",
    "l1 = torch.mean(y1**2)\n",
    "l1.backward()\n",
    "anagrad = x1.grad\n",
    "\n",
    "print('Analytical gradient: ', anagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing NN and Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(input_channels // 2, input_channels // 4, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(input_channels // 4, input_channels // 8, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(input_channels // 8, input_channels // 32, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (activation): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_cpu = Network(32)\n",
    "net_gpu = Network(32)\n",
    "net_gpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.Size([262144])\n",
      "17.6 ms ± 1.61 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "33.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "n = 512\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu')\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).view(-1)) # dry run\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).view(-1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).view(-1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.Size([1, 512, 512])\n",
      "19.6 ms ± 995 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "30 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# CPU with NDarray\n",
    "n = 512\n",
    "Pop = Transpose(dims=(1, n, n), axes=(0,2,1))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu')\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).squeeze(0))\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).squeeze(0))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).squeeze(0))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.Size([4, 262144])\n",
      "113 ms ± 139 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "148 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# CPU with batch \n",
    "n = 512\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu', batch=True)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((4, 32, n, n))).view(4, -1))\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_cpu(torch.ones((4, 32, n, n))).view(4, -1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((4, 32, n, n))).view(4, -1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.Size([4, 1, 512, 512])\n",
      "114 ms ± 9.89 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "170 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# CPU with batch NDarray\n",
    "n = 512\n",
    "Pop = Transpose(dims=(1, n, n), axes=(0,2,1))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu', batch=True, flatten=False)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((4, 32, n, n))))\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_cpu(torch.ones((4, 32, n, n))))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((4, 32, n, n))))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "11.4 ms ± 208 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "2.54 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_gpu = TorchOperator(Pop, device=device)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1)) # dry run\n",
    "print(y.device)\n",
    "%timeit -n2 -r2 Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 torch.Size([1, 512, 512])\n",
      "11.3 ms ± 78 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "2.64 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# GPU with NDarray\n",
    "n = 512\n",
    "Pop = Transpose(dims=(1, n, n), axes=(0,2,1))\n",
    "Pop_torch_gpu = TorchOperator(Pop, device=device)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).squeeze(0))\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).squeeze(0))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).squeeze(0))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 torch.Size([4, 262144])\n",
      "44.8 ms ± 126 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "3.68 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# GPU with batch \n",
    "n = 512\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_gpu = TorchOperator(Pop, device=device, batch=True)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((4, 32, n, n)).to(device)).view(4, -1))\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_gpu.apply(net_gpu(torch.ones((4, 32, n, n)).to(device)).view(4, -1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((4, 32, n, n)).to(device)).view(4, -1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 torch.Size([4, 1, 512, 512])\n",
      "44.9 ms ± 132 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "4.48 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# GPU with batch NDarray\n",
    "n = 512\n",
    "Pop = Transpose(dims=(1, n, n), axes=(0,2,1))\n",
    "Pop_torch_gpu = TorchOperator(Pop, device=device, batch=True, flatten=False)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((4, 32, n, n)).to(device)))\n",
    "print(y.device, y.shape)\n",
    "%timeit -n2 -r2 Pop_torch_gpu.apply(net_gpu(torch.ones((4, 32, n, n)).to(device)))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((4, 32, n, n)).to(device)))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8 ms ± 149 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "6.17 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Mixed\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu', devicetorch=device)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1)) # dry run\n",
    "print(y.device)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pylops_gpu_3090_torch1_10_1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "17f9aaec73ef57837ac82ec8cd1e0a65387e9eb595a33317b829a9862250a9db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
