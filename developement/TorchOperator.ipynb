{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pylops - torch operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: M.Ravasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will show how to use the `TorchOperator` to mix and match pylops and pytorch operators into an AD-friendy chain of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "from pylops.TorchOperator import TorchOperator\n",
    "from pylops.basicoperators import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 10])\n",
      "Input:  tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "AD gradient:  tensor([-0.6358,  0.3454, -0.8364, -0.7692, -3.0928, -0.3300,  1.9894, -0.1716,\n",
      "        -0.1775, -1.5061], dtype=torch.float64)\n",
      "Analytical gradient:  tensor([-0.6358,  0.3454, -0.8364, -0.7692, -3.0928, -0.3300,  1.9894, -0.1716,\n",
      "        -0.1775, -1.5061], dtype=torch.float64, grad_fn=<MvBackward>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "nx, ny = 10, 6\n",
    "x0 = torch.arange(nx, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx))\n",
    "Aop = TorchOperator(MatrixMult(A))\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "\n",
    "# AD\n",
    "v = torch.ones(ny, dtype=torch.double)\n",
    "y.backward(v, retain_graph=True)\n",
    "adgrad = x0.grad\n",
    "\n",
    "# Analytical\n",
    "At = torch.from_numpy(A)\n",
    "#J = (At * torch.cos(x0))\n",
    "J = (At * torch.cos(x0))\n",
    "print(J.shape)\n",
    "anagrad = torch.matmul(J.T, v)\n",
    "\n",
    "print('Input: ', x0)\n",
    "print('AD gradient: ', adgrad)\n",
    "print('Analytical gradient: ', anagrad)\n",
    "\n",
    "# Grad check\n",
    "input = (torch.arange(nx, dtype=torch.double, requires_grad=True),\n",
    "         Aop.matvec, Aop.rmatvec, Aop.device)\n",
    "test = gradcheck(Aop.Top, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi batch, we should get here to sum of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD gradient:  tensor([[-2.4999, -0.2820, -0.6562],\n",
      "        [ 2.4749,  0.3411,  0.4473],\n",
      "        [-2.4003, -0.3934, -0.2294],\n",
      "        [ 2.2777,  0.4379,  0.0070],\n",
      "        [-2.1096, -0.4736,  0.2156]])\n",
      "Analytical gradient:  tensor([[-2.4999, -0.2820, -0.6562],\n",
      "        [ 2.4749,  0.3411,  0.4473],\n",
      "        [-2.4003, -0.3934, -0.2294],\n",
      "        [ 2.2777,  0.4379,  0.0070],\n",
      "        [-2.1096, -0.4736,  0.2156]])\n"
     ]
    }
   ],
   "source": [
    "nbatch, nx, ny = 5, 3, 6\n",
    "x0 = torch.arange(nbatch * nx, dtype=torch.float).reshape(nbatch, nx)\n",
    "x0.requires_grad=True\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx)).astype(np.float32)\n",
    "Aop = TorchOperator(MatrixMult(A), batch=True)\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "\n",
    "# AD\n",
    "v = torch.ones((nbatch, ny), dtype=torch.float32)\n",
    "y.backward(v, retain_graph=True)\n",
    "adgrad = x0.grad\n",
    "print('AD gradient: ', adgrad)\n",
    "\n",
    "# Analytical\n",
    "x0.grad.data.zero_()\n",
    "At = torch.from_numpy(A)\n",
    "Lin = nn.Linear(nx, ny, bias=False)\n",
    "Lin.weight.data[:] = At.float()\n",
    "y1 = Lin(torch.sin(x0))\n",
    "y1.backward(v, retain_graph=True)\n",
    "anagrad = x0.grad\n",
    "\n",
    "print('Analytical gradient: ', anagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD gradient:  tensor([[-0.0396,  0.0809, -0.0171],\n",
      "        [-0.0822,  0.0764, -0.0187],\n",
      "        [-0.1198,  0.0615, -0.0130],\n",
      "        [-0.1494,  0.0376, -0.0005],\n",
      "        [-0.1688,  0.0064,  0.0179]])\n",
      "Analytical gradient:  tensor([[-0.0396,  0.0809, -0.0171],\n",
      "        [-0.0822,  0.0764, -0.0187],\n",
      "        [-0.1198,  0.0615, -0.0130],\n",
      "        [-0.1494,  0.0376, -0.0005],\n",
      "        [-0.1688,  0.0064,  0.0179]])\n"
     ]
    }
   ],
   "source": [
    "nbatch, nx, ny = 5, 3, 6\n",
    "x0 = torch.arange(nbatch*nx, dtype=torch.float).reshape(nbatch, nx).requires_grad_()\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx)).astype(np.float32)\n",
    "Aop = TorchOperator(MatrixMult(A), batch=True)\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "l = torch.mean(y**2)\n",
    "l.backward()\n",
    "adgrad = x0.grad\n",
    "print('AD gradient: ', adgrad)\n",
    "\n",
    "# Analytical\n",
    "x1 = torch.arange(nbatch*nx, dtype=torch.float).reshape(nbatch, nx).requires_grad_()\n",
    "At = torch.from_numpy(A)\n",
    "Lin = nn.Linear(nx, ny, bias=False)\n",
    "Lin.weight.data[:] = At.float()\n",
    "y1 = Lin(torch.sin(x1))\n",
    "l1 = torch.mean(y1**2)\n",
    "l1.backward()\n",
    "anagrad = x1.grad\n",
    "\n",
    "print('Analytical gradient: ', anagrad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
