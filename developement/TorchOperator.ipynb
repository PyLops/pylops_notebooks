{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pylops - torch operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: M.Ravasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will show how to use the `TorchOperator` to mix and match pylops and pytorch operators into an AD-friendy chain of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "from pylops.torchoperator import TorchOperator\n",
    "from pylops.basicoperators import *\n",
    "from pylops.signalprocessing import Convolve2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 10])\n",
      "Input:  tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "AD gradient:  tensor([ 1.0280, -2.7161,  1.3890, -3.7811, -1.3855, -0.7098, -1.7599, -1.7247,\n",
      "        -0.4403,  1.7002], dtype=torch.float64)\n",
      "Analytical gradient:  tensor([ 1.0280, -2.7161,  1.3890, -3.7811, -1.3855, -0.7098, -1.7599, -1.7247,\n",
      "        -0.4403,  1.7002], dtype=torch.float64, grad_fn=<MvBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "nx, ny = 10, 6\n",
    "x0 = torch.arange(nx, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx))\n",
    "Aop = TorchOperator(MatrixMult(A))\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "\n",
    "# AD\n",
    "v = torch.ones(ny, dtype=torch.double)\n",
    "y.backward(v, retain_graph=True)\n",
    "adgrad = x0.grad\n",
    "\n",
    "# Analytical\n",
    "At = torch.from_numpy(A)\n",
    "#J = (At * torch.cos(x0))\n",
    "J = (At * torch.cos(x0))\n",
    "print(J.shape)\n",
    "anagrad = torch.matmul(J.T, v)\n",
    "\n",
    "print('Input: ', x0)\n",
    "print('AD gradient: ', adgrad)\n",
    "print('Analytical gradient: ', anagrad)\n",
    "\n",
    "# Grad check\n",
    "input = (torch.arange(nx, dtype=torch.double, requires_grad=True),\n",
    "         Aop.matvec, Aop.rmatvec, Aop.device, 'cpu')\n",
    "test = gradcheck(Aop.Top, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi batch, we should get here to sum of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD gradient:  tensor([[-0.8326, -0.6849,  0.7844],\n",
      "        [ 0.8242,  0.8285, -0.5347],\n",
      "        [-0.7994, -0.9556,  0.2743],\n",
      "        [ 0.7586,  1.0636, -0.0083],\n",
      "        [-0.7026, -1.1503, -0.2577]])\n",
      "Analytical gradient:  tensor([[-0.8326, -0.6849,  0.7844],\n",
      "        [ 0.8242,  0.8285, -0.5347],\n",
      "        [-0.7994, -0.9556,  0.2743],\n",
      "        [ 0.7586,  1.0636, -0.0083],\n",
      "        [-0.7026, -1.1503, -0.2577]])\n"
     ]
    }
   ],
   "source": [
    "nbatch, nx, ny = 5, 3, 6\n",
    "x0 = torch.arange(nbatch * nx, dtype=torch.float).reshape(nbatch, nx)\n",
    "x0.requires_grad=True\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx)).astype(np.float32)\n",
    "Aop = TorchOperator(MatrixMult(A), batch=True)\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "\n",
    "# AD\n",
    "v = torch.ones((nbatch, ny), dtype=torch.float32)\n",
    "y.backward(v, retain_graph=True)\n",
    "adgrad = x0.grad\n",
    "print('AD gradient: ', adgrad)\n",
    "\n",
    "# Analytical\n",
    "x0.grad.data.zero_()\n",
    "At = torch.from_numpy(A)\n",
    "Lin = nn.Linear(nx, ny, bias=False)\n",
    "Lin.weight.data[:] = At.float()\n",
    "y1 = Lin(torch.sin(x0))\n",
    "y1.backward(v, retain_graph=True)\n",
    "anagrad = x0.grad\n",
    "\n",
    "print('Analytical gradient: ', anagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD gradient:  tensor([[-0.1963,  0.0531, -0.0243],\n",
      "        [-0.2584,  0.0653, -0.0231],\n",
      "        [-0.3078,  0.0750, -0.0150],\n",
      "        [-0.3404,  0.0815, -0.0005],\n",
      "        [-0.3537,  0.0842,  0.0190]])\n",
      "Analytical gradient:  tensor([[-0.1963,  0.0531, -0.0243],\n",
      "        [-0.2584,  0.0653, -0.0231],\n",
      "        [-0.3078,  0.0750, -0.0150],\n",
      "        [-0.3404,  0.0815, -0.0005],\n",
      "        [-0.3537,  0.0842,  0.0190]])\n"
     ]
    }
   ],
   "source": [
    "nbatch, nx, ny = 5, 3, 6\n",
    "x0 = torch.arange(nbatch*nx, dtype=torch.float).reshape(nbatch, nx).requires_grad_()\n",
    "\n",
    "# Forward\n",
    "A = np.random.normal(0., 1., (ny, nx)).astype(np.float32)\n",
    "Aop = TorchOperator(MatrixMult(A), batch=True)\n",
    "y = Aop.apply(torch.sin(x0))\n",
    "l = torch.mean(y**2)\n",
    "l.backward()\n",
    "adgrad = x0.grad\n",
    "print('AD gradient: ', adgrad)\n",
    "\n",
    "# Analytical\n",
    "x1 = torch.arange(nbatch*nx, dtype=torch.float).reshape(nbatch, nx).requires_grad_()\n",
    "At = torch.from_numpy(A)\n",
    "Lin = nn.Linear(nx, ny, bias=False)\n",
    "Lin.weight.data[:] = At.float()\n",
    "y1 = Lin(torch.sin(x1))\n",
    "l1 = torch.mean(y1**2)\n",
    "l1.backward()\n",
    "anagrad = x1.grad\n",
    "\n",
    "print('Analytical gradient: ', anagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing NN and Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(input_channels // 2, input_channels // 4, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(input_channels // 4, input_channels // 8, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(input_channels // 8, input_channels // 32, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (activation): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_cpu = Network(32)\n",
    "net_gpu = Network(32)\n",
    "net_gpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "17 ms ± 1.09 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "37.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "n = 512\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu')\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).view(-1)) # dry run\n",
    "print(y.device)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).view(-1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_cpu(torch.ones((1, 32, n, n))).view(-1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.2 ms ± 1.59 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "23.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_gpu = TorchOperator(Pop, device=device)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1)) # dry run\n",
    "print(y.device)\n",
    "%timeit -n2 -r2 Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_gpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n",
      "WARNING: pylops operator will be applied on the cpu whilst the input torch vector is on cuda:0, this may lead to poor performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 ms ± 547 µs per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
      "7.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Mixed (currently not allowed!)\n",
    "Pop = Transpose(dims=(n, n), axes=(1,0))\n",
    "Pop_torch_cpu = TorchOperator(Pop, device='cpu', devicetorch=device)\n",
    "\n",
    "# forward\n",
    "y = Pop_torch_cpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1)) # dry run\n",
    "print(y.device)\n",
    "%timeit -n2 -r2 Pop_torch_cpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "\n",
    "# backward\n",
    "y = Pop_torch_cpu.apply(net_gpu(torch.ones((1, 32, n, n)).to(device)).view(-1))\n",
    "loss = y.sum()\n",
    "%timeit -n1 -r1 loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pylops_gpu_3090_torch1_10_1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "17f9aaec73ef57837ac82ec8cd1e0a65387e9eb595a33317b829a9862250a9db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
