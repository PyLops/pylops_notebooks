{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pylops-GPU - extending pytorch with Lops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: M.Ravasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we experiment with extending Pytorch with PyLops linear operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pylops\n",
    "import pylops_gpu\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "from scipy.signal import triang\n",
    "from pylops import Diagonal, MatrixMult, FirstDerivative\n",
    "from pylops.utils import dottest\n",
    "from pylops import Restriction\n",
    "\n",
    "from scipy.sparse.linalg import cg\n",
    "from pylops_gpu import TorchOperator\n",
    "from pylops_gpu.utils.backend import device\n",
    "from pylops_gpu.utils import dottest as gdottest\n",
    "from pylops_gpu import Restriction as gRestriction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of scalar-scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following **scalar** input and output function:\n",
    "\n",
    "$$f(x) = (3*x)^2$$\n",
    "\n",
    "that is expressed as:\n",
    "\n",
    "$$y = 3*x, \\quad z = y^2$$\n",
    "\n",
    "We can thus compute the following derivatives\n",
    "\n",
    "$$df/dx = 18 * x, \\quad dy/dx = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = 3 * x\n",
    "z = y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n",
      "tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() # always clean gradient otherwise it will be summed\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.data.zero_() # always clean gradient otherwise it will be summed\n",
    "z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of vector-scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with a **scalar** function and a **vectorial** input\n",
    "\n",
    "$$f(\\textbf{x}) = \\sum (3*\\textbf{x})^2$$\n",
    "\n",
    "We can thus compute the following derivatives\n",
    "\n",
    "$$df/dx_i = 18 * x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(5, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * x\n",
    "z = torch.sum(y ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0., 18., 36., 54., 72.])\n"
     ]
    }
   ],
   "source": [
    "z.backward(retain_graph=True)\n",
    "#z.backward(torch.tensor(1.), retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of vector-vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider a **vectorial** function and a **vectorial** input\n",
    "\n",
    "$$\\textbf{y} = 3*\\textbf{x}^2$$\n",
    "\n",
    "Now we cannot compute the jacobian, but we can compute the product of the jacobian by a vector $$\\textbf{J}^T * \\textbf{v}$$.\n",
    "\n",
    "In our case:\n",
    "\n",
    "$$\\textbf{J} = \\begin{bmatrix}\n",
    "dy_1/dx_1&...&dy_N/dx_1 \\\\\n",
    "...&...&...\\\\\n",
    "dy_1/dx_N&...&dy_N/dx_M\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "6*x_1&...&0 \\\\\n",
    "...&...&...\\\\\n",
    "0&...&6*x_M\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we choose a unitary vector:\n",
    "\n",
    "$$\n",
    "\\textbf{g} = \\textbf{J}^T * \\textbf{v} = \\begin{vmatrix} 6*x_1 \\\\ ...\\\\ 6*x_M \\end{vmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(5, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * (x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.], requires_grad=True)\n",
      "tensor([ 0.,  6., 12., 18., 24.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(5)\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a **matrix-vector multiplication**\n",
    "\n",
    "$$\\textbf{y} = \\textbf{A}\\textbf{x}$$\n",
    "\n",
    "For any matrix the Jacobian is the matrix itself ($\\textbf{J} = \\textbf{A}$), and the gradient is equal:\n",
    "\n",
    "$$\\textbf{g} =\\textbf{A}^T\\textbf{v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 10, 5 \n",
    "A = torch.from_numpy(np.arange(n*m, dtype=np.float32).reshape(n, m))\n",
    "\n",
    "x = torch.arange(m, dtype=torch.float32, requires_grad=True)\n",
    "y = torch.matmul(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([225., 235., 245., 255., 265.])\n",
      "tensor([225., 235., 245., 255., 265.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(n)\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)\n",
    "print(torch.matmul(A.T, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we thus have the following relation:\n",
    "\n",
    "$$\\textbf{y} = \\textbf{A} (3*\\textbf{x}^2)$$\n",
    "\n",
    "the gradient can be obtained by first multiplying $\\textbf{A}^T$ followed by the gradient of the second term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30., 105., 180., 255., 330., 405., 480., 555., 630., 705.],\n",
       "       grad_fn=<MvBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = 10, 5 \n",
    "A = torch.from_numpy(np.arange(n*m, dtype=np.float32).reshape(n, m))\n",
    "\n",
    "x = torch.ones(m, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * x**2\n",
    "z = torch.matmul(A, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1350., 1410., 1470., 1530., 1590.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(n)\n",
    "v1 = torch.matmul(A.T, v)\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with full gradient from AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1350., 1410., 1470., 1530., 1590.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() # always clean gradient otherwise it will be summed\n",
    "z.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of linear operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider a linear operator that mimics a matrix $\\textbf{A}$ and define its backward operator as its adjoint and compare results with its equivalent dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMult(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, R):\n",
    "        y = torch.matmul(R, x)\n",
    "        ctx.save_for_backward(R)\n",
    "        return y\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, y):\n",
    "        R, = ctx.saved_tensors\n",
    "        return  torch.matmul(R.T, y), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(m, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * x**2\n",
    "z = MatMult.apply(y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1350., 1410., 1470., 1530., 1590.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(n)\n",
    "z.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a more complicated operator, the **Restriction** operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsampling \n",
    "perc_subsampling=0.4\n",
    "\n",
    "nsub=int(np.round(n*perc_subsampling))\n",
    "iava = np.sort(np.random.permutation(np.arange(n))[:nsub])\n",
    "R = np.zeros((nsub, n))\n",
    "R[np.arange(nsub), iava] = 1\n",
    "R = torch.from_numpy(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 6., 7., 8.], dtype=torch.float64, grad_fn=<MatMultBackward>)\n",
      "tensor([-0.0758,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8599,  1.8456,\n",
      "        -1.3833,  0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(n, dtype=torch.float64, requires_grad=True)\n",
    "y = MatMult.apply(x, R)\n",
    "print(y)\n",
    "\n",
    "# gradient\n",
    "v = torch.randn(nsub, dtype=torch.float64)\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we wrap the linear operator from pylops-gpu. \n",
    "\n",
    "This could become very generical way to include all linear operators of pylops in pytorch and create combination of NN layers and physical operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 6., 7., 8.], dtype=torch.float64, grad_fn=<_TorchOperatorBackward>)\n",
      "tensor([-0.0758,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8599,  1.8456,\n",
      "        -1.3833,  0.0000], dtype=torch.float64)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "Rop = gRestriction(n, iava, dtype=torch.float64)\n",
    "x = torch.arange(n, dtype=torch.float64, requires_grad=True)\n",
    "y = TorchOperator(Rop, pylops=False).apply(x)\n",
    "print(y)\n",
    "\n",
    "# gradient\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "inputs = (torch.randn(n, dtype=torch.double,requires_grad=True))\n",
    "test = gradcheck(TorchOperator(Rop).apply, inputs, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And directly using a pylops operator (conversion from and to torch Tensors is handled internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 6., 7., 8.], dtype=torch.float64, grad_fn=<_TorchOperatorBackward>)\n",
      "tensor([-0.0758,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8599,  1.8456,\n",
      "        -1.3833,  0.0000], dtype=torch.float64)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "Rop = Restriction(n, iava, dtype=np.float64)\n",
    "x = torch.arange(n, dtype=torch.float64, requires_grad=True)\n",
    "y = TorchOperator(Rop, pylops=True).apply(x)\n",
    "print(y)\n",
    "\n",
    "# gradient\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "inputs = (torch.randn(n, dtype=torch.double,requires_grad=True))\n",
    "test = gradcheck(TorchOperator(Rop).apply, inputs, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient on input of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0367,  0.0303,  0.1158, -0.0952,  0.0046, -0.0868, -0.1189, -0.0491,\n",
      "          0.0411, -0.1482]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 32, 32, requires_grad=True)\n",
    "out = net(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients on weights from vector output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.5344e-02,  7.4599e-02,  6.1744e-02],\n",
       "          [ 6.8918e-02,  7.0227e-02,  8.2948e-02],\n",
       "          [ 2.2136e-02,  4.4246e-02,  7.6388e-02]],\n",
       "\n",
       "         [[ 6.4809e-02,  1.1347e-01,  8.1592e-02],\n",
       "          [ 1.0299e-01,  8.3533e-02,  8.0949e-02],\n",
       "          [ 5.1546e-02,  4.8394e-02,  1.0211e-01]],\n",
       "\n",
       "         [[ 1.4843e-01,  9.3501e-02,  1.0322e-01],\n",
       "          [ 5.3313e-02,  1.5026e-01,  9.6460e-02],\n",
       "          [ 1.1382e-01,  1.0095e-01,  1.5473e-01]],\n",
       "\n",
       "         [[ 2.6541e-02,  3.9705e-02,  6.7609e-02],\n",
       "          [ 6.7996e-02,  4.0456e-02,  5.0336e-03],\n",
       "          [ 1.3968e-02,  6.8134e-02,  5.8479e-02]],\n",
       "\n",
       "         [[ 7.7581e-02,  1.2238e-01,  8.2076e-02],\n",
       "          [ 8.9508e-02,  7.0173e-02,  8.8250e-02],\n",
       "          [ 1.8027e-02,  7.7447e-02,  1.0314e-01]],\n",
       "\n",
       "         [[ 6.4214e-02,  6.7708e-02,  5.2168e-02],\n",
       "          [ 8.2369e-02,  1.1010e-01,  1.2574e-01],\n",
       "          [ 3.7718e-02,  3.0779e-02,  1.1155e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 2.6069e-02, -2.1718e-02,  1.4578e-03],\n",
       "          [-6.3751e-02, -2.9417e-02, -7.0326e-02],\n",
       "          [ 1.2953e-02,  1.4158e-02, -5.6487e-02]],\n",
       "\n",
       "         [[-1.7197e-02, -3.3068e-02,  8.9138e-03],\n",
       "          [-2.6525e-02, -4.5394e-02, -2.0892e-03],\n",
       "          [-5.9639e-02, -7.5666e-02, -2.4060e-02]],\n",
       "\n",
       "         [[-7.3281e-03,  3.4950e-02,  9.5701e-03],\n",
       "          [-5.6636e-02, -1.7428e-02, -1.4538e-02],\n",
       "          [-1.3130e-02, -1.0232e-01, -6.9408e-02]],\n",
       "\n",
       "         [[ 7.1219e-03,  2.5847e-02, -3.2883e-02],\n",
       "          [ 1.2273e-02, -2.0052e-02,  6.7603e-03],\n",
       "          [-7.1030e-02, -3.1777e-02, -2.8393e-02]],\n",
       "\n",
       "         [[-1.4674e-02, -7.0897e-02,  7.1132e-03],\n",
       "          [-4.6487e-03, -2.5397e-02,  3.2556e-02],\n",
       "          [-6.7694e-02, -6.9714e-02, -2.0932e-02]],\n",
       "\n",
       "         [[ 8.9889e-04,  2.3511e-02, -9.0838e-04],\n",
       "          [-5.9657e-02, -4.8915e-02,  1.2652e-03],\n",
       "          [-8.1813e-02, -4.2957e-02, -1.2025e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.0310e-02, -9.3687e-02, -5.5972e-02],\n",
       "          [-8.9872e-02, -1.9171e-02, -6.2553e-02],\n",
       "          [-7.9375e-02, -3.6923e-02, -2.0667e-02]],\n",
       "\n",
       "         [[-4.5499e-02, -7.5566e-02,  3.7209e-03],\n",
       "          [-7.0690e-02, -2.9579e-02, -7.5726e-02],\n",
       "          [-6.9020e-02, -2.6985e-02, -2.4217e-02]],\n",
       "\n",
       "         [[-3.9504e-02, -9.2664e-02, -5.7167e-02],\n",
       "          [-5.9448e-02, -6.1807e-02, -8.6266e-02],\n",
       "          [-7.8977e-02, -3.0993e-02, -4.2832e-02]],\n",
       "\n",
       "         [[-8.5845e-02, -1.5729e-02, -3.6531e-02],\n",
       "          [-3.0157e-02, -4.9931e-02,  8.3888e-03],\n",
       "          [-4.2710e-02, -5.3873e-02, -3.9614e-02]],\n",
       "\n",
       "         [[-3.9987e-02, -9.6166e-02,  1.3318e-02],\n",
       "          [-8.4086e-02, -5.1897e-02, -5.2944e-02],\n",
       "          [-4.9662e-02, -3.1413e-02, -3.7000e-02]],\n",
       "\n",
       "         [[-1.3823e-02, -5.9036e-02, -8.7968e-03],\n",
       "          [-1.5986e-03, -3.9602e-02, -7.0956e-02],\n",
       "          [-6.6947e-02, -4.1059e-02, -3.2217e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4111e-02,  3.3725e-02,  5.7659e-02],\n",
       "          [ 5.9592e-02,  1.8175e-02,  6.2158e-02],\n",
       "          [ 2.7681e-02,  2.7387e-02,  2.6515e-02]],\n",
       "\n",
       "         [[ 5.0243e-02,  7.0044e-02,  2.9939e-02],\n",
       "          [ 6.0991e-02,  3.0004e-02,  3.3406e-02],\n",
       "          [ 3.4278e-02,  3.7334e-02,  2.7125e-02]],\n",
       "\n",
       "         [[ 8.2579e-02,  2.8940e-02,  6.0372e-02],\n",
       "          [ 7.6036e-02,  5.4013e-02,  1.4472e-02],\n",
       "          [ 4.2916e-02,  1.9700e-03,  3.6506e-02]],\n",
       "\n",
       "         [[ 1.7800e-02,  5.2801e-02,  4.4944e-02],\n",
       "          [ 6.6129e-02,  2.8396e-02,  4.9884e-02],\n",
       "          [ 3.5342e-02,  2.7606e-02, -1.4999e-02]],\n",
       "\n",
       "         [[ 1.0937e-02,  4.8694e-02,  6.8969e-02],\n",
       "          [ 6.8015e-02,  2.2953e-02,  3.9434e-02],\n",
       "          [ 2.7364e-02,  5.5169e-02,  5.5632e-02]],\n",
       "\n",
       "         [[ 8.1734e-02, -3.7465e-02,  5.4079e-03],\n",
       "          [ 8.2473e-03,  2.8555e-03,  1.7132e-02],\n",
       "          [ 4.8514e-02,  3.9051e-02,  4.3392e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.6439e-02, -4.0083e-02, -6.6935e-02],\n",
       "          [ 2.2200e-05, -2.5006e-02, -1.5687e-02],\n",
       "          [-6.6184e-02, -5.2156e-04, -2.9739e-02]],\n",
       "\n",
       "         [[-7.9567e-02, -6.0503e-02,  3.8240e-03],\n",
       "          [-2.0865e-02,  8.3566e-03,  3.8748e-02],\n",
       "          [-1.8562e-02, -3.2981e-03, -6.2306e-03]],\n",
       "\n",
       "         [[-6.6549e-02, -2.8677e-02, -2.1496e-02],\n",
       "          [-4.8184e-02, -6.8776e-03,  5.3984e-02],\n",
       "          [ 1.1386e-02, -2.2603e-02, -2.6945e-02]],\n",
       "\n",
       "         [[-9.9674e-03, -3.6496e-02, -4.5784e-04],\n",
       "          [-3.5196e-02,  6.3896e-02, -2.5381e-02],\n",
       "          [-1.1326e-02, -2.7377e-02,  4.4827e-02]],\n",
       "\n",
       "         [[-3.7415e-02,  1.1606e-02, -1.1320e-02],\n",
       "          [ 5.9258e-03,  2.1341e-02, -1.6877e-02],\n",
       "          [-4.2677e-03, -2.8142e-02,  1.2604e-02]],\n",
       "\n",
       "         [[-4.9648e-02,  3.6730e-02,  6.2584e-02],\n",
       "          [-1.9892e-02,  4.9841e-02, -9.9015e-03],\n",
       "          [ 4.3854e-02,  5.9203e-03, -7.3393e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.5795e-02, -3.7384e-02, -3.7270e-02],\n",
       "          [-9.6044e-02, -2.3222e-02, -1.8980e-02],\n",
       "          [-3.1664e-02,  5.1695e-03, -1.5147e-02]],\n",
       "\n",
       "         [[-5.5186e-02, -4.5158e-02, -1.2516e-01],\n",
       "          [-9.3710e-03, -5.0303e-02, -9.4590e-02],\n",
       "          [-5.5872e-02, -8.3895e-02, -3.1711e-02]],\n",
       "\n",
       "         [[-8.2499e-02, -7.1632e-02, -1.2226e-01],\n",
       "          [-8.7092e-02, -5.9798e-02, -4.9674e-02],\n",
       "          [ 1.3253e-02, -1.2897e-01, -1.0532e-01]],\n",
       "\n",
       "         [[-2.6007e-02, -1.8581e-04, -1.1294e-02],\n",
       "          [ 5.9192e-03, -2.9196e-02, -6.8956e-03],\n",
       "          [-5.2311e-02, -2.3671e-03, -3.5099e-02]],\n",
       "\n",
       "         [[-6.1301e-02, -3.8879e-02, -6.5319e-02],\n",
       "          [ 4.7928e-03, -5.7094e-02, -9.6489e-02],\n",
       "          [-4.2448e-02, -7.7137e-02, -4.3966e-02]],\n",
       "\n",
       "         [[-5.0066e-02, -8.5353e-02, -6.1858e-02],\n",
       "          [ 2.5624e-02, -1.0923e-01, -1.3037e-01],\n",
       "          [ 9.1358e-03, -1.0038e-01, -8.1845e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.3524e-02,  2.1434e-02,  1.1676e-02],\n",
       "          [-3.7195e-02,  1.5579e-02, -5.2595e-03],\n",
       "          [-1.4141e-02,  7.9027e-03, -1.8132e-02]],\n",
       "\n",
       "         [[-6.3255e-03,  3.9126e-03,  1.4881e-02],\n",
       "          [-2.1137e-02, -1.2711e-02,  3.4608e-03],\n",
       "          [ 1.4169e-02, -5.4033e-02, -2.8026e-03]],\n",
       "\n",
       "         [[-2.7987e-02, -3.5161e-02, -8.0752e-03],\n",
       "          [-1.7174e-02, -2.7770e-02, -7.2614e-04],\n",
       "          [ 2.1364e-02, -3.7714e-02, -8.4669e-03]],\n",
       "\n",
       "         [[-4.6636e-02, -1.3793e-02,  3.9578e-03],\n",
       "          [ 8.5310e-03, -3.7298e-02, -3.3602e-02],\n",
       "          [-4.9556e-03,  4.9198e-03,  1.6235e-04]],\n",
       "\n",
       "         [[-4.5027e-03, -1.1861e-02,  6.7692e-03],\n",
       "          [-2.8296e-02, -1.8611e-02, -1.0419e-02],\n",
       "          [-1.4409e-02, -1.2868e-02,  3.5823e-04]],\n",
       "\n",
       "         [[ 1.8612e-02, -1.1217e-02, -1.5678e-02],\n",
       "          [ 8.2717e-03, -3.6105e-02, -1.8852e-02],\n",
       "          [ 1.7389e-02, -5.3563e-02, -2.9178e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0282e-02, -3.7607e-02, -2.5416e-02],\n",
       "          [ 3.8257e-02,  6.3064e-03, -1.3119e-02],\n",
       "          [ 5.2055e-02, -4.3886e-02, -1.9067e-02]],\n",
       "\n",
       "         [[-1.4826e-04, -2.1480e-02, -4.0364e-02],\n",
       "          [-1.6669e-02, -1.4539e-02, -6.0065e-04],\n",
       "          [ 2.9125e-03, -3.1146e-02, -1.9010e-02]],\n",
       "\n",
       "         [[-2.3789e-02, -1.3286e-02, -9.8599e-04],\n",
       "          [ 4.8047e-03, -3.9663e-02, -6.7754e-02],\n",
       "          [-2.9632e-02, -1.1309e-02, -3.2639e-02]],\n",
       "\n",
       "         [[-1.0874e-02,  3.1360e-02,  1.0900e-02],\n",
       "          [-2.1541e-02, -1.9783e-02, -1.3204e-02],\n",
       "          [ 1.5586e-02,  3.3976e-02,  1.1414e-02]],\n",
       "\n",
       "         [[-2.6886e-02, -2.1925e-02, -5.3817e-03],\n",
       "          [-3.1561e-02, -2.2447e-02, -2.0360e-02],\n",
       "          [-9.6377e-03,  4.7389e-03, -3.9779e-02]],\n",
       "\n",
       "         [[-2.2516e-02, -2.7102e-02, -4.7746e-02],\n",
       "          [-9.9703e-03, -2.0796e-02,  3.3690e-04],\n",
       "          [-5.1929e-02,  1.7412e-02, -7.1128e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.3843e-02, -3.0932e-02,  6.7345e-04],\n",
       "          [ 1.4601e-02, -5.7227e-02, -3.0991e-02],\n",
       "          [ 2.1373e-02, -3.2890e-02, -3.2595e-02]],\n",
       "\n",
       "         [[-3.8989e-02, -1.9203e-02,  1.8979e-02],\n",
       "          [-1.7513e-02, -5.6333e-03,  4.9968e-03],\n",
       "          [-2.3472e-02, -1.6015e-02, -5.6208e-02]],\n",
       "\n",
       "         [[-1.2704e-02, -7.2614e-03, -1.3386e-02],\n",
       "          [-3.8031e-02,  1.4468e-02,  9.7287e-03],\n",
       "          [-2.1517e-02, -1.6279e-02,  6.2248e-03]],\n",
       "\n",
       "         [[-1.2469e-02, -3.3165e-02,  1.6886e-02],\n",
       "          [ 1.9866e-02, -1.5976e-02, -4.3136e-02],\n",
       "          [-9.0896e-03, -1.6172e-02, -1.7530e-03]],\n",
       "\n",
       "         [[-3.3460e-02, -1.0718e-02, -4.2253e-04],\n",
       "          [ 5.5070e-03, -4.0772e-03, -1.0478e-02],\n",
       "          [-2.2231e-02, -7.1548e-03, -2.1840e-03]],\n",
       "\n",
       "         [[-3.1639e-02, -3.5662e-04,  2.6022e-02],\n",
       "          [-5.6572e-02, -5.9868e-03,  3.0535e-02],\n",
       "          [-3.4246e-02, -1.9112e-02,  1.2065e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.1641e-03, -1.4969e-03, -1.0022e-03],\n",
       "          [-1.4660e-03, -4.5593e-03, -2.6048e-03],\n",
       "          [-2.6367e-03, -3.5958e-03, -1.7547e-03]],\n",
       "\n",
       "         [[-1.7091e-03, -7.1239e-04, -1.0118e-03],\n",
       "          [-3.2824e-03, -8.0402e-04, -3.8951e-03],\n",
       "          [ 2.6658e-05, -3.6900e-03, -2.6240e-03]],\n",
       "\n",
       "         [[-4.2393e-03, -1.5572e-03, -2.3680e-03],\n",
       "          [-5.1808e-03, -3.0493e-03, -4.4100e-03],\n",
       "          [-1.7362e-03, -3.5491e-03, -2.2790e-03]],\n",
       "\n",
       "         [[-1.5661e-03, -2.0992e-03,  9.3514e-05],\n",
       "          [-1.1267e-03,  8.6939e-05, -3.6603e-03],\n",
       "          [-4.2725e-03, -2.6266e-03,  1.5140e-04]],\n",
       "\n",
       "         [[-2.5842e-03, -1.0072e-03, -1.3413e-03],\n",
       "          [-4.2596e-03, -2.6442e-03, -2.8646e-03],\n",
       "          [-1.6639e-03, -2.6070e-03, -1.0897e-03]],\n",
       "\n",
       "         [[-3.8265e-03, -4.8508e-04, -2.9796e-03],\n",
       "          [-2.4090e-03, -8.6948e-04, -3.4256e-03],\n",
       "          [ 1.9698e-04, -1.3019e-03, -3.1980e-03]]],\n",
       "\n",
       "\n",
       "        [[[-4.2875e-02, -3.8917e-02,  4.9641e-03],\n",
       "          [-8.7297e-02, -3.5723e-02, -7.9086e-02],\n",
       "          [-8.8234e-02, -9.2339e-02, -1.6226e-02]],\n",
       "\n",
       "         [[-5.1331e-02, -6.2282e-02, -7.8829e-02],\n",
       "          [-5.1479e-02, -9.1698e-02, -7.3994e-02],\n",
       "          [-8.1689e-02, -3.3532e-02, -5.0417e-02]],\n",
       "\n",
       "         [[-1.0185e-01, -5.3950e-02, -5.4694e-02],\n",
       "          [-1.0272e-01, -1.0428e-01, -1.3502e-01],\n",
       "          [-8.4731e-02, -4.0113e-02, -6.2897e-02]],\n",
       "\n",
       "         [[-1.2847e-02, -3.5968e-02, -7.2275e-02],\n",
       "          [-3.1159e-02, -6.5661e-02,  2.2815e-04],\n",
       "          [-1.0376e-03, -3.3064e-02, -3.2856e-02]],\n",
       "\n",
       "         [[-2.6049e-02, -8.5157e-02, -6.8832e-02],\n",
       "          [-5.4876e-02, -7.0996e-02, -7.5199e-02],\n",
       "          [-2.5005e-02, -3.4799e-02, -7.0024e-02]],\n",
       "\n",
       "         [[-6.6750e-02, -1.1861e-01, -8.2321e-02],\n",
       "          [-3.0474e-02, -1.0402e-01, -4.8942e-02],\n",
       "          [-8.5838e-02, -2.7665e-02, -1.0705e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 9.6996e-02,  4.2037e-02,  6.4717e-02],\n",
       "          [ 1.9008e-02,  4.9724e-02,  2.3545e-02],\n",
       "          [-2.9657e-02,  3.0847e-02,  2.5519e-02]],\n",
       "\n",
       "         [[ 5.9196e-02, -8.4686e-03,  4.4776e-02],\n",
       "          [ 7.4351e-02,  2.3614e-02,  5.6034e-02],\n",
       "          [ 3.1844e-02,  6.2369e-02,  4.2480e-02]],\n",
       "\n",
       "         [[ 1.1523e-03,  5.2227e-02,  3.5686e-02],\n",
       "          [ 4.7787e-02,  1.1131e-02,  5.0448e-02],\n",
       "          [ 6.2485e-02,  4.1511e-02,  6.4680e-02]],\n",
       "\n",
       "         [[ 3.7475e-02,  4.0967e-02,  2.7232e-03],\n",
       "          [-1.1144e-02,  3.0332e-02,  6.2128e-03],\n",
       "          [-1.9935e-03,  3.9024e-02,  1.7083e-02]],\n",
       "\n",
       "         [[ 2.8485e-02,  1.9320e-02,  1.9604e-02],\n",
       "          [ 5.0443e-02,  9.5017e-03,  5.6039e-02],\n",
       "          [ 4.1275e-02,  3.5254e-02,  2.2548e-02]],\n",
       "\n",
       "         [[-1.5514e-05, -1.8692e-03,  5.2899e-03],\n",
       "          [ 3.6157e-02,  7.5664e-03,  4.7005e-02],\n",
       "          [ 6.8650e-02,  6.6016e-02,  2.4603e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.1023e-02,  7.9786e-02, -3.6740e-03],\n",
       "          [ 3.7888e-02,  4.9108e-02, -2.1251e-02],\n",
       "          [-1.4490e-02, -6.5637e-03, -1.1991e-02]],\n",
       "\n",
       "         [[ 3.1315e-02,  4.1693e-02, -1.7376e-02],\n",
       "          [-3.8746e-02, -2.5139e-02,  3.0906e-02],\n",
       "          [ 4.4436e-02, -1.6961e-03, -1.7861e-02]],\n",
       "\n",
       "         [[ 8.9938e-03,  4.5091e-02,  4.0357e-02],\n",
       "          [-5.8344e-02, -6.0008e-02,  4.2835e-02],\n",
       "          [ 2.7089e-02, -1.3638e-02, -3.7457e-03]],\n",
       "\n",
       "         [[ 2.5696e-02, -9.0483e-04, -1.6838e-03],\n",
       "          [-4.7130e-02, -4.6904e-03, -3.1417e-03],\n",
       "          [-1.2393e-03,  2.7657e-02, -2.0789e-02]],\n",
       "\n",
       "         [[ 4.8718e-02,  2.0735e-02, -3.3121e-03],\n",
       "          [-3.4105e-02,  4.8678e-03,  2.4645e-02],\n",
       "          [ 3.1547e-02, -1.4672e-02,  7.9238e-03]],\n",
       "\n",
       "         [[ 1.7360e-02, -2.4258e-02, -3.0985e-02],\n",
       "          [ 2.4001e-02, -1.0417e-02,  2.2092e-02],\n",
       "          [ 3.8958e-02,  1.4896e-02,  1.1329e-03]]],\n",
       "\n",
       "\n",
       "        [[[-4.1287e-02,  6.5758e-02,  3.9372e-02],\n",
       "          [-6.1689e-02,  4.3376e-03,  1.9783e-02],\n",
       "          [-1.6747e-02,  2.7660e-02,  4.0874e-02]],\n",
       "\n",
       "         [[ 1.2636e-02, -2.0791e-02, -2.6956e-02],\n",
       "          [ 7.6442e-03, -3.1527e-02, -2.2024e-03],\n",
       "          [-3.4171e-02, -6.8575e-04,  2.0259e-02]],\n",
       "\n",
       "         [[-1.7364e-02, -3.7364e-02, -3.1483e-02],\n",
       "          [ 3.2207e-03, -4.1793e-02, -7.3249e-02],\n",
       "          [-1.2965e-02, -4.4923e-03, -2.2718e-02]],\n",
       "\n",
       "         [[-1.9465e-02,  1.3178e-02,  2.8666e-02],\n",
       "          [ 1.5961e-02, -1.6241e-02,  1.5006e-02],\n",
       "          [ 1.8734e-02, -4.4586e-04, -1.7807e-02]],\n",
       "\n",
       "         [[ 5.5024e-03, -2.4850e-02, -1.6062e-02],\n",
       "          [-4.8292e-03, -2.5687e-02, -1.5198e-02],\n",
       "          [ 1.0370e-02,  1.7523e-03, -1.4557e-02]],\n",
       "\n",
       "         [[ 2.2748e-02, -3.1556e-02, -9.9854e-02],\n",
       "          [ 3.2717e-02, -3.2866e-02, -7.6061e-02],\n",
       "          [-1.8216e-02, -5.4236e-02, -3.1021e-02]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "net.conv2.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients on weights from scalar output (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6220, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "target = torch.randn(10).view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x1c2e1a0208>\n",
      "<AddmmBackward object at 0x1c2e1a01d0>\n",
      "<AccumulateGrad object at 0x1c2e1a0208>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0058,  0.0100, -0.0100,  0.0096,  0.0008, -0.0016])\n",
      "x.grad after backward\n",
      "tensor([[[[ 2.3789e-04,  2.8962e-04,  6.5590e-04,  ..., -8.3606e-05,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-1.0139e-03,  8.9873e-04, -1.0501e-03,  ...,  7.2838e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-3.1228e-04,  1.6695e-03,  8.7161e-04,  ..., -2.6210e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.8365e-04,  3.1709e-04,  2.2372e-04,  ...,  1.7347e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "print('x.grad after backward')\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.7889e-04, -2.0635e-04, -2.3091e-04,  ..., -1.0745e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-4.1266e-04,  3.6506e-04, -7.1320e-04,  ...,  8.8593e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-6.0151e-04,  2.6901e-03,  3.7990e-05,  ..., -5.2463e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 1.4937e-03,  4.9296e-04,  1.5585e-04,  ...,  3.5435e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "v = torch.autograd.Variable(torch.from_numpy(np.random.normal(0, 1, [1, 10]).astype(np.float32)))\n",
    "output.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
