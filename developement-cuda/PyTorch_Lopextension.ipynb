{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pylops-GPU - extending pytorch with Lops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: M.Ravasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we experiment with extending Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pylops\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "from scipy.signal import triang\n",
    "from pylops import Diagonal, MatrixMult, FirstDerivative\n",
    "from pylops.utils import dottest\n",
    "from pylops.signalprocessing import Convolve1D\n",
    "\n",
    "from scipy.sparse.linalg import cg\n",
    "from pylops_gpu.utils.backend import device\n",
    "from pylops_gpu.utils import dottest as gdottest\n",
    "from pylops_gpu import Restriction as gRestriction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following **scalar** function:\n",
    "\n",
    "$$f(x) = (3*x)^2$$\n",
    "\n",
    "that is expressed as:\n",
    "\n",
    "$$y = 3*x, \\quad z = y^2$$\n",
    "\n",
    "We can thus compute the following derivatives\n",
    "\n",
    "$$df/dx = 18 * x, \\quad dy/dx = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = 3 * x\n",
    "z = y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n",
      "tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() # always clean gradient otherwise it will be summed\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.data.zero_() # always clean gradient otherwise it will be summed\n",
    "z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with a **scalar** function and a **vectorial** input\n",
    "\n",
    "$$f(\\textbf{x}) = \\sum (3*\\textbf{x})^2$$\n",
    "\n",
    "We can thus compute the following derivatives\n",
    "\n",
    "$$df/dx_i = 18 * x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(5, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * x\n",
    "z = torch.sum(y ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0., 18., 36., 54., 72.])\n"
     ]
    }
   ],
   "source": [
    "z.backward(torch.tensor(1.), retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider a **vectorial** function and a **vectorial** input\n",
    "\n",
    "$$\\textbf{y} = 3*\\textbf{x}^2$$\n",
    "\n",
    "Now we cannot compute the jacobian, but we can compute the product of the jacobian by a vector $$\\textbf{J}^T * \\textbf{v}$$.\n",
    "\n",
    "In our case:\n",
    "\n",
    "$$\\textbf{J} = \\begin{vmatrix}\n",
    "dy_1/dx_1&...&dy_1/dx_M \\\\\n",
    "...&...&...\\\\\n",
    "dy_N/dx_1&...&dy_N/dx_M\n",
    "\\end{vmatrix} = \n",
    "\\begin{vmatrix}\n",
    "6*x_1&...&0 \\\\\n",
    "...&...&...\\\\\n",
    "0&...&6*x_M\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "If we choose a unitary vector:\n",
    "\n",
    "$$\n",
    "\\textbf{g} = \\textbf{J}^T * \\textbf{v} = \\begin{vmatrix} 6*x_1 \\\\ ...\\\\ 6*x_M \\end{vmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(5, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * (x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.], requires_grad=True)\n",
      "tensor([ 0.,  6., 12., 18., 24.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(5)\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a **matrix-vector multiplication**\n",
    "\n",
    "$$\\textbf{y} = \\textbf{A}\\textbf{x}$$\n",
    "\n",
    "For any matrix the Jacobian is the matrix itself ($\\textbf{J} = \\textbf{A}$), and the gradient is equal:\n",
    "\n",
    "$$\\textbf{g} =\\textbf{A}^T\\textbf{v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 10, 5 \n",
    "A = torch.from_numpy(np.arange(n*m, dtype=np.float32).reshape(n, m))\n",
    "\n",
    "x = torch.arange(m, dtype=torch.float32, requires_grad=True)\n",
    "y = torch.matmul(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([225., 235., 245., 255., 265.])\n",
      "tensor([225., 235., 245., 255., 265.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(n)\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)\n",
    "print(torch.matmul(A.T, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we thus have the following relation:\n",
    "\n",
    "$$\\textbf{y} = \\textbf{A} (3*\\textbf{x}^2)$$\n",
    "\n",
    "the gradient can be obtained by first multiplying $\\textbf{A}^T$ followed by the gradient of the second term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30., 105., 180., 255., 330., 405., 480., 555., 630., 705.],\n",
       "       grad_fn=<MvBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = 10, 5 \n",
    "A = torch.from_numpy(np.arange(n*m, dtype=np.float32).reshape(n, m))\n",
    "\n",
    "x = torch.ones(m, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * x**2\n",
    "z = torch.matmul(A, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1350., 1410., 1470., 1530., 1590.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.matmul(A.T, torch.ones(n))\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with full gradient from AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1350., 1410., 1470., 1530., 1590.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() # always clean gradient otherwise it will be summed\n",
    "v = torch.ones(n)\n",
    "z.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we consider a linear operator that mimics a matrix $\\textbf{A}$ and define its backward operator as its adjoint and compare results with its equivalent dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMult(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, R):\n",
    "        y = torch.matmul(R, x)\n",
    "        ctx.save_for_backward(R)\n",
    "        return y\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, y):\n",
    "        R, = ctx.saved_tensors\n",
    "        return  torch.matmul(R.T, y), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(m, dtype=torch.float32, requires_grad=True)\n",
    "y = 3 * x**2\n",
    "z = MatMult.apply(y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1350., 1410., 1470., 1530., 1590.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.ones(n)\n",
    "z.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a more complicated operator, the **Restriction** operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lop(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, iava):\n",
    "        #save_for_backward(iava, torch.Tensor([x.shape[0]]))\n",
    "        ctx.iava = iava\n",
    "        ctx.n = x.shape[0]\n",
    "        y = torch.take(x,iava)\n",
    "        #y =x[iava]\n",
    "        return y\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, y):\n",
    "        #iava, n = ctx.saved_tensors\n",
    "        x = torch.zeros(int(ctx.n),  dtype=torch.float64)\n",
    "        x[ctx.iava] = y\n",
    "        return  x, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsampling \n",
    "perc_subsampling=0.4\n",
    "\n",
    "nsub=int(np.round(n*perc_subsampling))\n",
    "iava = np.sort(np.random.permutation(np.arange(n))[:nsub])\n",
    "R = np.zeros((nsub, n))\n",
    "R[np.arange(nsub), iava] = 1\n",
    "R = torch.from_numpy(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 6.], dtype=torch.float64, grad_fn=<MatMultBackward>)\n",
      "tensor([ 0.0000, -0.2306, -1.6422, -1.0424,  0.0000,  0.0000,  0.1976,  0.0000,\n",
      "         0.0000,  0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(n, dtype=torch.float64, requires_grad=True)\n",
    "y = MatMult.apply(x, R)\n",
    "print(y)\n",
    "\n",
    "# gradient\n",
    "v = torch.randn(nsub, dtype=torch.float64)\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 6.], dtype=torch.float64, grad_fn=<LopBackward>)\n",
      "tensor([ 0.0000, -0.2306, -1.6422, -1.0424,  0.0000,  0.0000,  0.1976,  0.0000,\n",
      "         0.0000,  0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Rop = gRestriction(n, iava, dtype=torch.float64)\n",
    "x = torch.arange(n, dtype=torch.float64, requires_grad=True)\n",
    "y = Lop.apply(x, torch.from_numpy(iava))\n",
    "print(y)\n",
    "\n",
    "# gradient\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "inputs = (torch.randn(n,dtype=torch.double,requires_grad=True), torch.from_numpy(iava))\n",
    "test = gradcheck(Lop.apply, inputs, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And wrapping the methods from pylops. This could become very generical way to include all linear operators of pylops in pytorch and create combination of NN layers and physical operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lop(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, forw, adj):\n",
    "        ctx.forw = forw\n",
    "        ctx.adj = adj\n",
    "        y = ctx.forw(x)\n",
    "        return y\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, y):\n",
    "        x = ctx.adj(y)\n",
    "        return  x, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 6.], dtype=torch.float64, grad_fn=<LopBackward>)\n",
      "tensor([ 0.0000, -0.2306, -1.6422, -1.0424,  0.0000,  0.0000,  0.1976,  0.0000,\n",
      "         0.0000,  0.0000], dtype=torch.float64)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "Rop = gRestriction(n, iava, dtype=torch.float64)\n",
    "x = torch.arange(n, dtype=torch.float64, requires_grad=True)\n",
    "y = Lop.apply(x, Rop.matvec, Rop.rmatvec)\n",
    "print(y)\n",
    "\n",
    "# gradient\n",
    "y.backward(v, retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "inputs = (torch.randn(n,dtype=torch.double,requires_grad=True), Rop.matvec, Rop.rmatvec)\n",
    "test = gradcheck(Lop.apply, inputs, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
