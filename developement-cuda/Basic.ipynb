{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xaygeqc0YgF7"
      },
      "source": [
        "# Pylops - CUDA basic linear operators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "owGwScKKYgGA"
      },
      "source": [
        "### Author: M.Ravasi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUIuNUfYYgGC"
      },
      "source": [
        "In this notebook we will experiment with Pytorch to assess its usability as backend for CUDA enabled operators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hMwmxTTxZ4Ju",
        "outputId": "07ee2172-cd24-41d4-e960-3f71e6b53b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        }
      },
      "source": [
        "!pip install pylops\n",
        "!pip install git+https://git@github.com/equinor/pylops-gpu.git@master"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pylops\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/5a/dc9d93cd0f9ba3ea9a77c30c92865f07523ebf2fc391dff19aeca2f2b848/pylops-1.4.0-py3-none-any.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from pylops) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pylops) (1.3.0)\n",
            "Installing collected packages: pylops\n",
            "Successfully installed pylops-1.4.0\n",
            "Collecting git+https://git@github.com/equinor/pylops-gpu.git@master\n",
            "  Cloning https://git@github.com/equinor/pylops-gpu.git (to revision master) to /tmp/pip-req-build-tzuejl6s\n",
            "  Running command git clone -q https://git@github.com/equinor/pylops-gpu.git /tmp/pip-req-build-tzuejl6s\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from pylops-gpu==0.0.0) (1.16.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pylops-gpu==0.0.0) (1.1.0)\n",
            "Collecting pytorch_complex_tensor (from pylops-gpu==0.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/98/6e5718dd0c7d7b648c560624aea9a81b98b0442e03a7ff4c81430b0c8082/pytorch-complex-tensor-0.0.134.tar.gz\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_complex_tensor->pylops-gpu==0.0.0) (0.3.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->pytorch_complex_tensor->pylops-gpu==0.0.0) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->pytorch_complex_tensor->pylops-gpu==0.0.0) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.2.1->pytorch_complex_tensor->pylops-gpu==0.0.0) (0.46)\n",
            "Building wheels for collected packages: pylops-gpu, pytorch-complex-tensor\n",
            "  Building wheel for pylops-gpu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ap4whnkx/wheels/c7/f0/0b/513e9be2dad0cbe2a900c0414a94bcae8778093383364aab9a\n",
            "  Building wheel for pytorch-complex-tensor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/9c/1c/35f0a149f864e1b4d5f0711bf76745aebef1f306fc8ab9ec40\n",
            "Successfully built pylops-gpu pytorch-complex-tensor\n",
            "Installing collected packages: pytorch-complex-tensor, pylops-gpu\n",
            "Successfully installed pylops-gpu-0.0.0 pytorch-complex-tensor-0.0.134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86gLYP4HYgGI",
        "outputId": "9eb21c98-ad9a-499e-94aa-f4fd8c1706f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "%pylab inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pylops\n",
        "from pylops import Diagonal\n",
        "from pylops.utils import dottest\n",
        "from pylops_gpu.utils.backend import device\n",
        "from pylops_gpu.utils import dottest as gdottest\n",
        "from pylops_gpu import Diagonal as gDiagonal"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5psZx5uvNM77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bebdc996-5605-4bbd-abfb-fd30b4032d01"
      },
      "source": [
        "dev = device()\n",
        "print('PyLops-gpu working on %s...' % dev)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyLops-gpu working on cuda...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tW1zPopNUUk",
        "colab_type": "text"
      },
      "source": [
        "## Diagonal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXpLuro4Nalr",
        "colab_type": "text"
      },
      "source": [
        "Example with model and data already on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS0QPHLwFutW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fe4aee28-0592-456d-f9d2-7e42b5036c82"
      },
      "source": [
        "n = 100000\n",
        "xg = torch.ones(n, dtype=torch.float32).to(dev)\n",
        "dg = (torch.arange(0, n, dtype=torch.float32) + 1.).to(dev)\n",
        "\n",
        "x = xg.cpu().numpy()\n",
        "d = dg.cpu().numpy()\n",
        "\n",
        "Dop = Diagonal(d)\n",
        "Dop_gpu = gDiagonal(dg, device=dev)\n",
        "dottest(Dop, n, n, verb=True)\n",
        "gdottest(Dop_gpu, n, n, device=dev, verb=True)\n",
        "\n",
        "# y = Dx\n",
        "yg = Dop_gpu * xg\n",
        "print(yg)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dot test passed, v^T(Opu)=-7843362.240638 - u^T(Op^Tv)=-7843362.240638\n",
            "Dot test passed, v^T(Opu)=1246191872.000000 - u^T(Op^Tv)=1246191872.000000\n",
            "tensor([1.0000e+00, 2.0000e+00, 3.0000e+00,  ..., 9.9998e+04, 9.9999e+04,\n",
            "        1.0000e+05], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XER4rcF2Ni23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b7c90ef0-b804-4604-8bc1-4151a4642864"
      },
      "source": [
        "%timeit -n 10 Dop * x\n",
        "%timeit -n 10 Dop_gpu * xg"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 101 µs per loop\n",
            "10 loops, best of 3: 12.7 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msimlxTDNn8U",
        "colab_type": "text"
      },
      "source": [
        "Example with model and data transfered from and to gpu in forward and adjoint operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfytb17lNLZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e2453a49-a836-497b-df44-625a872ad5fe"
      },
      "source": [
        "n = 100000\n",
        "xg = torch.ones(n, dtype=torch.float32).to(dev)\n",
        "dg = (torch.arange(0, n, dtype=torch.float32) + 1.).to(dev)\n",
        "\n",
        "xc = xg.cpu()\n",
        "x = xg.cpu().numpy()\n",
        "d = dg.cpu().numpy()\n",
        "\n",
        "Dop = Diagonal(d)\n",
        "Dop_gpu = gDiagonal(dg, device=dev, togpu=(True, True), tocpu=(True, True))\n",
        "gdottest(Dop_gpu, n, n, verb=True)\n",
        "\n",
        "# y = Dx\n",
        "y = Dop_gpu * x\n",
        "# xinv = D^-1 y\n",
        "xinv = Dop_gpu / y\n",
        "\n",
        "print(y)\n",
        "print(xinv)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dot test passed, v^T(Opu)=1245179904.000000 - u^T(Op^Tv)=1245179904.000000\n",
            "[1.0000e+00 2.0000e+00 3.0000e+00 ... 9.9998e+04 9.9999e+04 1.0000e+05]\n",
            "[4.09996035e-07 1.63998405e-06 3.68996179e-06 ... 9.99280264e-01\n",
            " 9.99063708e-01 9.98822355e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXJkOrNtHMGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63abc448-3651-4b42-8273-5413580cdf54"
      },
      "source": [
        "Dop_gpu = gDiagonal(dg, device=dev, togpu=(True, True), tocpu=(True, True))\n",
        "\n",
        "%timeit -n 10 Dop * x\n",
        "%timeit -n 10 Dop_gpu * xc"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 116 µs per loop\n",
            "10 loops, best of 3: 363 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo-JeU7iPSc4",
        "colab_type": "text"
      },
      "source": [
        "Note here how we get beaten by the cost of moving x and y back and forth between CPU and GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9UsaZGZhccU",
        "colab_type": "text"
      },
      "source": [
        "# 1D Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7K9pSgihQNP",
        "colab_type": "code",
        "outputId": "089a8fda-cb75-404d-ef6f-c9c398f4e765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "N = 11\n",
        "Nh = 3\n",
        "x = np.zeros(N)\n",
        "x[N//2] = 1\n",
        "\n",
        "h = np.arange(Nh)+1\n",
        "y = np.convolve(x, h, mode='same')\n",
        "print(y)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 2. 3. 0. 0. 0. 0.]\n",
            "(11,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hqf1yHfhHqU",
        "colab_type": "code",
        "outputId": "8fa1ace3-bd55-40c0-856a-0bd5290b1e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "xt = torch.zeros(N)\n",
        "xt[N//2] = 1\n",
        "\n",
        "ht = torch.torch.arange(0, Nh, dtype=torch.float) + 1.\n",
        "yt = torch.torch.conv_transpose1d(xt.reshape(1, 1, N), ht.reshape(1, 1, 3), padding=Nh//2)\n",
        "print(yt)\n",
        "print(yt.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 1., 2., 3., 0., 0., 0., 0.]]])\n",
            "torch.Size([1, 1, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y7V0DRVkojI",
        "colab_type": "code",
        "outputId": "c5742571-24f0-4c49-f7b3-8be2739b3f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y = np.correlate(x, h, mode='same')\n",
        "print(y)\n",
        "\n",
        "yt = torch.torch.conv1d(xt.reshape(1, 1, N), ht.reshape(1, 1, 3), padding=Nh//2)\n",
        "print(yt)\n",
        "print(yt.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 3. 2. 1. 0. 0. 0. 0.]\n",
            "tensor([[[0., 0., 0., 0., 3., 2., 1., 0., 0., 0., 0.]]])\n",
            "torch.Size([1, 1, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxgUNx98haUX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgdPeJb0iCy1",
        "colab_type": "code",
        "outputId": "cf8284d3-a4cb-4091-85d7-a6db7a59cbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "xt = torch.zeros((1000, N))\n",
        "xt[:, N//2] = 1\n",
        "ht = torch.torch.arange(0, Nh, dtype=torch.float) + 1.\n",
        "\n",
        "xc = xt.to(device)\n",
        "hc = ht.to(device)\n",
        "\n",
        "yt = torch.torch.conv1d(xt.reshape(1000, 1, N), ht.reshape(1, 1, 3), padding=Nh//2)\n",
        "print(yt.shape)\n",
        "yc = torch.torch.conv1d(xc.reshape(1000, 1, N), hc.reshape(1, 1, 3), padding=Nh//2)\n",
        "print(yc.shape)\n",
        "\n",
        "% timeit torch.torch.conv1d(xt.reshape(1000, 1, N), ht.reshape(1, 1, 3), padding=Nh//2)\n",
        "% timeit torch.torch.conv1d(xc.reshape(1000, 1, N), hc.reshape(1, 1, 3), padding=Nh//2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 1, 11])\n",
            "torch.Size([1000, 1, 11])\n",
            "1000 loops, best of 3: 434 µs per loop\n",
            "The slowest run took 15.01 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 3: 40.6 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}