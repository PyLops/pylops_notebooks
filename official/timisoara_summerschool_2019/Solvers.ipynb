{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solvers\n",
    "\n",
    "This notebook will focus on various least-squares and L1 solvers and show how an apparently ill-posed problem can be solved by adding additional (prior) information to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylops\n",
    "import scooby\n",
    "\n",
    "from scipy.linalg import lstsq, toeplitz, cholesky\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from pylops import LinearOperator\n",
    "from pylops.utils import dottest\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we are going to consider is generally referred to\n",
    "as the *data reconstruction* problem and aims at reconstructing a regularly\n",
    "sampled signal of size $M$ from $N$ randomly selected samples:\n",
    "\n",
    "\\begin{align}\\mathbf{d} = \\mathbf{R} \\mathbf{m}\\end{align}\n",
    "\n",
    "where the restriction operator $\\mathbf{R}$ that selects the $M$\n",
    "elements from $\\mathbf{m}$ at random locations is implemented using `pylops.Restriction`, and\n",
    "\n",
    "\\begin{align}\\mathbf{d}= [d_1, d_2,...,d_N]^T, \\qquad\n",
    "    \\mathbf{m}= [m_1, m_2,...,m_M]^T, \\qquad\\end{align}\n",
    "\n",
    "with $M>>N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create the data in the frequency domain. The data is composed\n",
    "by the superposition of 3 sinusoids with different frequencies. We then use the `pylops.FFT` operator to obtain the time domain version of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input signal parameters\n",
    "ifreqs = [41, 25, 66]\n",
    "amps = [1., 1., 1.]\n",
    "nt = 200\n",
    "nfft = 2**11\n",
    "dt = 0.004\n",
    "t = np.arange(nt)*dt\n",
    "f = np.fft.rfftfreq(nfft, dt)\n",
    "\n",
    "# input signal in frequency domain\n",
    "X = np.zeros(nfft//2+1, dtype='complex128')\n",
    "X[ifreqs] = amps\n",
    "\n",
    "# input signal in time domain\n",
    "FFTop = pylops.signalprocessing.FFT(nt, nfft=nfft, real=True)\n",
    "x = FFTop.H*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the locations at which the signal will be sampled, apply forward, adjoint and inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling locations    \n",
    "perc_subsampling = 0.2\n",
    "ntsub = int(np.round(nt*perc_subsampling))\n",
    "iava = np.sort(np.random.permutation(np.arange(nt))[:ntsub])\n",
    "\n",
    "# create operator\n",
    "Rop = pylops.Restriction(nt, iava, dtype='float64')\n",
    "\n",
    "# create noise\n",
    "sigma=1e-2\n",
    "n = np.random.normal(0, sigma, nt)\n",
    "\n",
    "# apply forward\n",
    "y = Rop*x\n",
    "yn = Rop*(x + n)\n",
    "ymask = Rop.mask(x)\n",
    "ynmask = Rop.mask(x + n)\n",
    "\n",
    "# apply adjoint\n",
    "xadj = Rop.H*yn\n",
    "\n",
    "# apply inverse\n",
    "xinv = Rop / yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "axs[0].plot(f, np.abs(X), 'k', lw=4)\n",
    "axs[0].set_xlim(0, 30)\n",
    "axs[0].set_title('Signal (frequency domain)')\n",
    "axs[1].plot(t, x, 'k', lw=4)\n",
    "axs[1].plot(t, ymask, '.k', ms=25, label='available samples')\n",
    "axs[1].plot(t, ynmask, '.r', ms=25, label='available noisy samples')\n",
    "axs[1].plot(t, xinv, 'r', lw=3, label='inverted samples')\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Signal (time domain)')\n",
    "axs[1].set_xlim(0, 0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that solving the problem without providing any additional (prior) information does not provide a satisfactory estimate of the original (finely sampled) signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we add a second term to the cost function imposing minimum second order derivative of the model. This is equivalent to minimizing the roughness of the reconstruction, leading to a smooth estimate:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "            \\mathbf{R}    \\\\\n",
    "            \\epsilon \\nabla\n",
    "\\end{bmatrix} \\mathbf{m} =\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{d}    \\\\\n",
    "    \\mathbf{0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2op = pylops.SecondDerivative(nt, dims=None, dtype='float64')\n",
    "epsR = np.sqrt(1e1)\n",
    "epsI = np.sqrt(1e-4)\n",
    "\n",
    "xne = \\\n",
    "    pylops.optimization.leastsquares.RegularizedInversion(Rop, [D2op], yn,\n",
    "                                                          epsRs=[epsR],\n",
    "                                                          returninfo=False,\n",
    "                                                          **dict(iter_lim=100, damp=epsI))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(t, x, 'k', lw=6)\n",
    "ax.plot(t, ymask, '.k', ms=25, label='available samples')\n",
    "ax.plot(t, ynmask, '.r', ms=25, label='available noisy samples')\n",
    "ax.plot(t, xne, 'r', lw=3, label='inverted samples')\n",
    "ax.legend()\n",
    "ax.set_title('Regularized inversion')\n",
    "ax.set_xlim(0, 0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now the same problem, but we frame in a Bayesian context. We have two equivalent solutions:\n",
    "\n",
    "$$\\mathbf{m} = (\\mathbf{R}^T \\mathbf{C}^{-1}_d \\mathbf{R} + \\mathbf{C}^{-1}_m)^{-1} (\\mathbf{R}^T \\mathbf{C}^{-1}_d \\mathbf{d} + \\mathbf{C}^{-1}_m \\mathbf{m_0})$$\n",
    "\n",
    "and  \n",
    "\n",
    "$$\\mathbf{m} = \\mathbf{m_0} + (\\mathbf{R} \\mathbf{C}_m \\mathbf{R}^T + \\mathbf{C}_d)^{-1} (\\mathbf{d} - \\mathbf{R} \\mathbf{m_0})$$\n",
    "\n",
    "The first one can be easily written as a regularized problem:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "            \\mathbf{C}^{-1/2}_d \\mathbf{R}    \\\\\n",
    "            \\mathbf{C}^{-1/2}_{m} \\\\\n",
    "\\end{bmatrix} \\mathbf{m} =\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{C}^{-1/2}_d \\mathbf{d}    \\\\\n",
    "    \\mathbf{C}^{-1/2}_{m} \\mathbf{m}_0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "but it is difficult to implement because we would need to be able to 'model' the square-root inverse covariances.\n",
    "\n",
    "On the other hand the first one can be easily implemented using linear operators, where:\n",
    "\n",
    "\n",
    "$$\\mathbf{C}_d = \\sigma^2 \\mathbf{I} \\quad (\\mathbf{C}^{-1/2}_d  = 1/\\sigma \\mathbf{I})$$,\n",
    "\n",
    "$$\\mathbf{C}_m = \\mathbf{C}_m^{\\sigma^{1/2}} \\mathbf{C}_m^{\\rho} \\mathbf{C}_m^{\\sigma^{1/2}} \\quad (\\mathbf{C}^{-1/2}_m  = (\\mathbf{C}_m^{\\sigma^{-1/2} })^T \\mathbf{C}_m^{\\rho^{-1/2}})$$,\n",
    "\n",
    "$$\\mathbf{m}_0 = m_0 \\mathbf{I}$$\n",
    "\n",
    "Here $\\mathbf{C}_m^{\\sigma^{1/2}} = \\sigma_m \\mathbf{I}$ and $\\mathbf{C}_m^{\\rho} = (\\mathbf{C}_m^{\\rho^{1/2}})^T \\mathbf{C}_m^{\\rho^{1/2}}$, where $\\mathbf{C}_m^{\\rho^{1/2}}$ is a convolutional matrix with a given correlation shape (e.g., exponentional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we deal with the model prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "x0 = 0.1 * np.ones_like(x)\n",
    "\n",
    "# Correlation and covariance functions\n",
    "corrrange = .1\n",
    "sigma_m = 1e-2\n",
    "taxis = np.arange(nt)*dt\n",
    "\n",
    "corr_sqrt = np.exp(-3*(taxis)**2/corrrange**2)\n",
    "Corr_sqrt = toeplitz(corr_sqrt)\n",
    "Corr = np.dot(Corr_sqrt.T, Corr_sqrt)\n",
    "Std = np.diag(sigma_m*np.ones(Corr.shape[0]))\n",
    "Cm = np.dot(Std, np.dot(Corr, Std))\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(Corr_sqrt, interpolation='nearest')\n",
    "ax1.set_title(r\"$\\mathbf{C}_m^{\\rho^{1/2}}$\")\n",
    "ax1.axis('tight')\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.imshow(Corr, interpolation='nearest')\n",
    "ax2.set_title(r\"$\\mathbf{C}_m^{\\rho}$\")\n",
    "ax2.axis('tight')\n",
    "\n",
    "# Inverse\n",
    "Corr_sqrt_inv = np.linalg.pinv(Corr_sqrt, rcond=1e-10)\n",
    "Corr_inv = np.linalg.pinv(Corr, rcond=1e-10)\n",
    "Cm_inv = np.linalg.pinv(Cm, rcond=1e-10)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(Cm_inv, interpolation='nearest')\n",
    "ax1.set_title(r\"$\\mathbf{C}_m^{-1}$\")\n",
    "ax1.axis('tight')\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(Cm_inv[nt//2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by solving the problem explicitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem\n",
    "R = np.zeros((ntsub, nt))\n",
    "R[np.arange(ntsub), iava] = 1\n",
    "\n",
    "# Data covariance\n",
    "Cd = sigma**2 * np.eye(ntsub)\n",
    "\n",
    "xbayes = \\\n",
    "    x0 + np.dot(Cm, np.dot(R.T, np.dot(np.linalg.inv(np.dot(R, np.dot(Cm, R.T)) + Cd), \n",
    "                                        yn - np.dot(R, x0))))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(t, x, 'k', lw=6)\n",
    "ax.plot(t, ymask, '.k', ms=25, label='available samples')\n",
    "ax.plot(t, ynmask, '.r', ms=25, label='available noisy samples')\n",
    "ax.plot(t, xbayes, 'r', lw=3, label='inverted samples')\n",
    "ax.legend()\n",
    "ax.set_title('Bayesian inversion')\n",
    "ax.set_xlim(0, 0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we however do not want to create those matrices explicitely we create equivalent linear operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cm_op = \\\n",
    "    (sigma_m * pylops.Identity(nt)) * \\\n",
    "    pylops.signalprocessing.Convolve1D(nt, Corr[nt//2, nt//2-50:nt//2+51], offset=50) * \\\n",
    "    (sigma_m * pylops.Identity(nt))\n",
    "Cd_op = (sigma**2) * pylops.Identity(ntsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbayes = x0 + Cm_op * Rop.H * (lsqr(Rop * Cm_op * Rop.H + Cd_op, yn - Rop*x0, iter_lim=100, damp=1e-5)[0])\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(t, x, 'k', lw=6)\n",
    "ax.plot(t, ymask, '.k', ms=25, label='available samples')\n",
    "ax.plot(t, ynmask, '.r', ms=25, label='available noisy samples')\n",
    "ax.plot(t, xbayes, 'r', lw=3, label='inverted samples')\n",
    "ax.set_title('Bayesian inversion with lops')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconditioned inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to deterministic inversion, we add now a preconditioner imposing smoothness in the solution:\n",
    "\n",
    "$$\\mathbf{d} = \\mathbf{R}  \\mathbf{S} \\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{m} = \\mathbf{P} \\mathbf{z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sop  = pylops.basicoperators.Smoothing1D(nsmooth=21, dims=[nt], dtype='float64')\n",
    "\n",
    "xprec = \\\n",
    "    pylops.optimization.leastsquares.PreconditionedInversion(Rop, Sop, yn, returninfo=False,\n",
    "                                                             **dict(damp=np.sqrt(1e-3), iter_lim=100, show=0))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(t, x, 'k', lw=6)\n",
    "ax.plot(t, ymask, '.k', ms=25, label='available samples')\n",
    "ax.plot(t, ynmask, '.r', ms=25, label='available noisy samples')\n",
    "ax.plot(t, xprec, 'r', lw=3, label='inverted samples')\n",
    "ax.legend()\n",
    "ax.set_title('Preconditioned inversion')\n",
    "ax.set_xlim(0, 0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude we use a sparse solver. We assume knowledge about the modelling process: the input signal was created in the frequency domain as summation of three sinusoids. We can thus assume the signal Fourier representation will be sparse (i.e, just a few spikes at the correct frequencies). \n",
    "\n",
    "In mathematical terms, solving for a sparse model is equivalent to add a regularization terms that requires to minimize the $L_1$ norm of the model:\n",
    "\n",
    "$$ ||\\mathbf{p}||_1 \\qquad subject \\quad to \\qquad  \\mathbf{d} = \\mathbf{R} \\mathbf{S} \\mathbf{z} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylops.Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse inversion\n",
    "xspgl1, zspgl1, info = pylops.optimization.sparsity.SPGL1(Rop, yn, FFTop, tau=3, iter_lim=100)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(t, x, 'k', lw=6)\n",
    "ax.plot(t, ymask, '.k', ms=25, label='available samples')\n",
    "ax.plot(t, ynmask, '.r', ms=25, label='available noisy samples')\n",
    "ax.plot(t, xspgl1, 'r', lw=3, label='inverted samples')\n",
    "ax.legend()\n",
    "ax.set_title('Sparse inversion')\n",
    "ax.set_xlim(0, 0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby.Report(core='pylops')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
